{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load measurement data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readBonsai(path):\n",
    "    bonsai = pd.read_csv(path)\n",
    "    bonsai = bonsai[['accX', 'accY', 'accZ', 'gyrX', 'gyrY', 'gyrZ']]\n",
    "    return bonsai\n",
    "    \n",
    "def readEXLS3(path):\n",
    "    exl = pd.read_fwf(path)\n",
    "    exl.columns = exl.iloc[2]\n",
    "    exl = exl[['a_x [g]:', 'a_y [g]:', 'a_z [g]:', 'ar_x [rad/s]:', 'ar_y [rad/s]:', 'ar_z [rad/s]:']]\n",
    "    exl.rename(index=int, columns={\n",
    "        'a_x [g]:': 'accX', 'a_y [g]:': 'accY', 'a_z [g]:': 'accZ', \n",
    "        'ar_x [rad/s]:': 'gyrX', 'ar_y [rad/s]:': 'gyrY', 'ar_z [rad/s]:': 'gyrZ'\n",
    "    }, inplace=True)\n",
    "    exl = exl.iloc[3:]\n",
    "    exl.reset_index(drop=True, inplace=True)\n",
    "    exl = exl.apply(pd.to_numeric)\n",
    "    exl = exl.multiply(9.80665)\n",
    "    return exl\n",
    "\n",
    "def tagColumnNames(df, tag):\n",
    "    newColumnNames = {columnName: columnName + tag for columnName in df.columns}\n",
    "    return df.rename(index=int, columns=newColumnNames)\n",
    "\n",
    "\n",
    "fileNameLocationMap = {\n",
    "    'I-L9H': 'hip-r',\n",
    "    'I-74V': 'hip-l',\n",
    "    'I-WXB': 'knee-r',\n",
    "    'I-0GN': 'knee-l',\n",
    "    'Gait - R': 'foot-r',\n",
    "    'Gait - L': 'foot-l'\n",
    "}\n",
    "def mapFileNameToLocation(fileName):\n",
    "    for name, location in fileNameLocationMap.items():\n",
    "        if (name in fileName):\n",
    "            return location\n",
    "    return 'unknown'\n",
    "\n",
    "def loadMeasurements(path):\n",
    "    measurements = {}\n",
    "    for fileOrDir in os.listdir(path):\n",
    "        if (fileOrDir.endswith('.txt')):\n",
    "            measurement = readEXLS3(os.path.join(path, fileOrDir))\n",
    "        elif (fileOrDir.endswith('.csv')):\n",
    "            measurement = readBonsai(os.path.join(path, fileOrDir))\n",
    "        if (measurement is not None):\n",
    "            measurementLocation = mapFileNameToLocation(fileOrDir)\n",
    "            measurement = tagColumnNames(measurement, '_' + measurementLocation)\n",
    "            measurements[measurementLocation] = measurement\n",
    "    return measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroMovementWindowSize = 200 # 10ms * zeroMovement\n",
    "\n",
    "def calibrate(series):\n",
    "    zeroWindowIndex = series.abs().rolling(zeroMovementWindowSize).median().sort_values().index[0]\n",
    "    zero = series.rolling(zeroMovementWindowSize).median().iloc[zeroWindowIndex]\n",
    "    series -= zero "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronize the sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfJumps = 3\n",
    "jumpBinSize = 50 # 10ms * jumpBinSize = time per bin; bundles neighbor values to avoid multiple amplitudes during same jump\n",
    "jumpSequenceLength = 800 # 10 ms * jumpSequenceLength\n",
    "relativeMaxThreshold = 7 / 12\n",
    "\n",
    "def binMeasurement(measurement, binSize):\n",
    "    absMeasurement = measurement\n",
    "    return absMeasurement.groupby(pd.cut(absMeasurement.index, np.arange(absMeasurement.index[0], absMeasurement.index[len(absMeasurement) - 1], binSize))).max()\n",
    "\n",
    "def findJumpingWindow(measurement):\n",
    "    measurement = measurement.head(int(len(measurement) / 2)) # jumping should be in first half\n",
    "    absMeasurement = measurement.abs()\n",
    "    threshold = absMeasurement.max() * relativeMaxThreshold\n",
    "    absMeasurement = absMeasurement.apply(lambda value: value if value >= threshold else 0)\n",
    "    bins = binMeasurement(absMeasurement, jumpBinSize).reset_index().drop('index', axis='columns')\n",
    "    upperBound = bins.rolling(int(jumpSequenceLength / jumpBinSize)).sum().iloc[:,0].sort_values(ascending=False).index[0]\n",
    "    lowerBound = upperBound - int(jumpSequenceLength / jumpBinSize)\n",
    "    upperBound *= jumpBinSize\n",
    "    lowerBound *= jumpBinSize\n",
    "    return max(lowerBound - 100, 0), min(upperBound + 100, len(measurement) - 1)\n",
    "\n",
    "def getFirstJumpIndex(measurement):\n",
    "    windowIndicies = findJumpingWindow(measurement)\n",
    "    window = measurement[windowIndicies[0]:  windowIndicies[1]]\n",
    "    threshold = window.max() * relativeMaxThreshold\n",
    "    window = window.apply(lambda value: 1 if value >= threshold else 0)\n",
    "    return window.loc[window == 1].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignSignals(dfX, dfY):\n",
    "    return getFirstJumpIndex(dfX) - getFirstJumpIndex(dfY)\n",
    "\n",
    "def alignAccelerationYWithRightFoot(measurements, location, axis):\n",
    "    offset = alignSignals(\n",
    "        measurements['foot-r']['accY_foot-r'], \n",
    "        measurements[location]['acc' + axis.upper() + '_' + location])\n",
    "    measurements[location] = measurements[location].shift(offset, axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroMovementThreshold = 0.75 # given in meters per second\n",
    "\n",
    "def getNextBinaryBlock(series, startPosition, minSubsequentMovements, zeroMode=True):\n",
    "    start = series[startPosition:]\n",
    "    if (zeroMode):\n",
    "        start = start[series == 0]\n",
    "    else:\n",
    "        start = start[series == 1]\n",
    "    if (len(start) == 0):\n",
    "        raise ValueError\n",
    "    start = start.index[0]\n",
    "    iValue = start\n",
    "    zeroCounter = 0\n",
    "    while (iValue < len(series)):\n",
    "        if (not series[iValue]):\n",
    "            zeroCounter += 1\n",
    "            iValue += 1\n",
    "        elif (zeroCounter < minSubsequentMovements):\n",
    "            return getNextBinaryBlock(series, iValue + 1, minSubsequentMovements)\n",
    "        else:\n",
    "            break\n",
    "    return start, iValue - 1\n",
    "\n",
    "def findAllNonZeroBlocks(series, startPosition, minSubsequentZeroMovements=200, minSubsequentNonZeroMovements=29, ignoreMinSubsequentNonZeroMovements=True):\n",
    "    '''\n",
    "    Finds all blocks of movement (expects a filtered list with 1s and 0s, gives back indices of 1-blocks).\n",
    "    Thresholds:\n",
    "    - minSubsequentZeroMovements: minimal length of zero blocks to interrupt movement blocks\n",
    "    - minSubsequentNonZeroMovements: minimal length of movement blocks\n",
    "    - ignoreMinSubsequentNonZeroMovements: if minSubsequentNonZeroMovements should be ignored\n",
    "    '''\n",
    "    blocks = []\n",
    "    start = series[startPosition:][series == 1].index[0]\n",
    "    while (start < len(series)):\n",
    "        try:\n",
    "            zeroStart, zeroEnd = getNextBinaryBlock(series, start, minSubsequentZeroMovements)\n",
    "            if ((((zeroStart - 1) - start) > minSubsequentNonZeroMovements) or ignoreMinSubsequentNonZeroMovements):\n",
    "                blocks.append((start, zeroStart - 1))\n",
    "            start = zeroEnd + 1\n",
    "        except ValueError:\n",
    "            if ((((len(series) - 1) - start) > minSubsequentNonZeroMovements) or ignoreMinSubsequentNonZeroMovements):\n",
    "                blocks.append((start, len(series) - 1))\n",
    "            start = len(series)\n",
    "    return blocks\n",
    "\n",
    "def splitDataFrameIntoExercises(df, columnName):\n",
    "    measurement = df[columnName]\n",
    "    windowIndicies = findJumpingWindow(measurement)\n",
    "    filteredByTH = measurement.abs().apply(lambda value: 1 if value > zeroMovementThreshold else 0)\n",
    "    exerciseIntervals = findAllNonZeroBlocks(filteredByTH, windowIndicies[1])\n",
    "    return list(map(lambda interval: df[interval[0] : interval[1]], exerciseIntervals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restingThreshold=0.75 # given in m/s\n",
    "minRestingInterval = 25 # we are taking the resting intervals of the right foot to detect ends of strides\n",
    "minMovementInterval = 5 # movementIntervals seperate the resting intervals, we are not looking for them\n",
    "borderStrideSpacing = 10 # spacing for beginning of first and end of last stride to start and end of measurement to avoid incomplete strides\n",
    "\n",
    "def findRestingBlocks(series):\n",
    "    filteredByTH = series.abs().apply(lambda value: 1 if value < restingThreshold else 0).reset_index(drop=True)\n",
    "    return findAllNonZeroBlocks(filteredByTH, 0, minSubsequentZeroMovements=minMovementInterval, minSubsequentNonZeroMovements=minRestingInterval, ignoreMinSubsequentNonZeroMovements=False)\n",
    "\n",
    "def findFirstStride(series, nextStrides):\n",
    "    firstRestingInterval = findRestingBlocks(series)[0]\n",
    "    if (firstRestingInterval[0] > borderStrideSpacing and nextStrides[0][0] > firstRestingInterval[0]):\n",
    "        return (firstRestingInterval[0], nextStrides[0][0])\n",
    "\n",
    "def findStrideIntervals(series):\n",
    "    restingIntervals = findRestingBlocks(series)\n",
    "    strideIntervals = []\n",
    "    for i in range(len(restingIntervals) - 1):\n",
    "        if (restingIntervals[i][1] < restingIntervals[i+1][1]):\n",
    "            strideIntervals.append((restingIntervals[i][1], restingIntervals[i+1][1]))\n",
    "    if (len(series) - strideIntervals[-1][1] < borderStrideSpacing):\n",
    "        strideIntervals = strideIntervals[:-1]\n",
    "    return strideIntervals\n",
    "\n",
    "def splitExerciseIntoStrides(df):\n",
    "    measurement = df['accY_foot-r']\n",
    "    otherFoot = df['accY_foot-l']\n",
    "    strideIntervals = findStrideIntervals(measurement)\n",
    "    # in case of complete first stride being present but starting with left foot,\n",
    "    # take its start until first already measured stride\n",
    "    firstStride = findFirstStride(otherFoot, strideIntervals)\n",
    "    if (firstStride):\n",
    "        strideIntervals = [firstStride] + strideIntervals\n",
    "    splittedExercise = [df]\n",
    "    splittedExercise += list(map(lambda interval: df[interval[0] : interval[1]], strideIntervals))\n",
    "    return splittedExercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizedStrideLength = 150\n",
    "\n",
    "def interpolateStride(stride):\n",
    "    difference = normalizedStrideLength - len(stride)\n",
    "    lowerHalf = math.floor(difference/2)\n",
    "    upperHalf = math.ceil(difference/2)\n",
    "    return stride.reindex(range(-lowerHalf, len(stride) + upperHalf)).fillna(0)\n",
    "    \n",
    "    \n",
    "def resampleStride(stride):\n",
    "    absStride = stride\n",
    "    return absStride.groupby(pd.cut(absStride.index, np.linspace(absStride.index[0], absStride.index[len(absStride) - 1], normalizedStrideLength + 1))).median()\n",
    "\n",
    "def normalizeStrides(strides):\n",
    "    '''\n",
    "    bring strides to same length by interpolating strides that are too short and resampling strides that are too long\n",
    "    expects a list of stride dataframes\n",
    "    '''\n",
    "    for i, stride in enumerate(strides):\n",
    "        if (len(stride) > normalizedStrideLength):\n",
    "            strides[i] = resampleStride(stride)\n",
    "        elif (len(stride) < normalizedStrideLength):\n",
    "            strides[i] = interpolateStride(stride)\n",
    "    return strides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine date with calibration and sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minExerciseLength = 300 # 10ms * minExerciseLength\n",
    "expectedExerciseCount = 6\n",
    "\n",
    "def alignAll(measurements):\n",
    "    alignAccelerationYWithRightFoot(measurements, 'hip-r', 'y')\n",
    "    alignAccelerationYWithRightFoot(measurements, 'hip-l', 'y')\n",
    "    alignAccelerationYWithRightFoot(measurements, 'foot-l', 'y')\n",
    "    alignAccelerationYWithRightFoot(measurements, 'knee-l', 'y')\n",
    "    alignAccelerationYWithRightFoot(measurements, 'knee-r', 'Y')\n",
    "    \n",
    "def calibrateAll(measurements):\n",
    "    for location in measurements.values():\n",
    "        for column in location.columns:\n",
    "            calibrate(location[column])\n",
    "            \n",
    "def resetTimePointZero(mergedDf):\n",
    "    firstIndex = max([mergedDf[column].first_valid_index() for column in mergedDf])\n",
    "    lastIndex = min([mergedDf[column].last_valid_index() for column in mergedDf])\n",
    "    return mergedDf[firstIndex:lastIndex]\n",
    "        \n",
    "def loadSyncedMeasurements(path):\n",
    "    measurements = loadMeasurements(path)\n",
    "    calibrateAll(measurements)\n",
    "    alignAll(measurements)\n",
    "    mergedDf = pd.DataFrame()\n",
    "    for measurement in measurements.values():\n",
    "        mergedDf = mergedDf.join(measurement, how='outer')\n",
    "    mergedDf = resetTimePointZero(mergedDf).reset_index().drop('index', axis='columns')\n",
    "    exercisesAndTurns = splitDataFrameIntoExercises(mergedDf, 'accY_foot-r')\n",
    "    exercises = list(filter(lambda exerciseOrTurn: len(exerciseOrTurn) > minExerciseLength, exercisesAndTurns))\n",
    "    if (len(exercises) is not expectedExerciseCount):\n",
    "        print(\"Unexpected exercise count: \", len(exercises))\n",
    "    data = [mergedDf] + exercises\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity of Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Time Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTWDistance(s1, s2, w):\n",
    "    DTW={}\n",
    "    \n",
    "    w = max(w, abs(len(s1)-len(s2)))\n",
    "    \n",
    "    for i in range(-1,len(s1)):\n",
    "        for j in range(-1,len(s2)):\n",
    "            DTW[(i, j)] = float('inf')\n",
    "    DTW[(-1, -1)] = 0\n",
    "  \n",
    "    for i in range(len(s1)):\n",
    "        for j in range(max(0, i-w), min(len(s2), i+w)):\n",
    "            dist= (s1[i]-s2[j])**2\n",
    "            DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n",
    "\n",
    "    return math.sqrt(DTW[len(s1)-1, len(s2)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faster Similarity with Lower Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LB_Keogh(s1, s2, r):\n",
    "    LB_sum=0\n",
    "    for ind,i in enumerate(s1):\n",
    "        \n",
    "        lower_bound=min(s2[(ind-r if ind-r>=0 else 0):(ind+r)])\n",
    "        upper_bound=max(s2[(ind-r if ind-r>=0 else 0):(ind+r)])\n",
    "        \n",
    "        if i>upper_bound:\n",
    "            LB_sum=LB_sum+(i-upper_bound)**2\n",
    "        elif i<lower_bound:\n",
    "            LB_sum=LB_sum+(i-lower_bound)**2\n",
    "    \n",
    "    return math.sqrt(LB_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def knnForSensor(train, test, w):\n",
    "    sensorResults = []\n",
    "    for testSeries in test:\n",
    "            min_dist=float('inf')\n",
    "            closest_seq=[]\n",
    "            for trainSeries in train:\n",
    "                if LB_Keogh(testSeries[:-1], trainSeries[:-1],5) < min_dist:\n",
    "                    dist=DTWDistance(testSeries[:-1], trainSeries[:-1], w)\n",
    "                    if dist < min_dist:\n",
    "                        min_dist=dist\n",
    "                        closest_seq=trainSeries\n",
    "            sensorResults.append(closest_seq[-1])\n",
    "    return sensorResults\n",
    "\n",
    "def collectVotesForStride(knnResults, strideIndex):\n",
    "    strideVotingResults = []\n",
    "    for key in knnResults.keys():\n",
    "        cluster = knnResults[key][strideIndex]\n",
    "        votingIndex = next((index for (index, vote) in enumerate(strideVotingResults) if vote['cluster'] == cluster), None)\n",
    "        if (type(votingIndex) is int):\n",
    "            strideVotingResults[votingIndex]['count'] += 1\n",
    "        else:\n",
    "            strideVotingResults.append({'cluster': cluster, 'count': 1})\n",
    "    return strideVotingResults\n",
    "\n",
    "def voteOnResults(knnResults):\n",
    "    votingResults = []\n",
    "    for strideIndex in range(len(knnResults[list(knnResults.keys())[0]])):\n",
    "        strideVotes = collectVotesForStride(knnResults, strideIndex)\n",
    "        mostFrequentVote = max(strideVotes, key=lambda x:x['count'])\n",
    "        votingResults.append(mostFrequentVote['cluster'])\n",
    "    return votingResults\n",
    "\n",
    "def knn(trainStrides, testStrides, w):\n",
    "    knnResults = {key:[] for key in trainStrides.keys()}\n",
    "    \n",
    "    for key in trainStrides:\n",
    "        train = trainStrides[key]\n",
    "        test = testStrides[key]\n",
    "        knnResults[key] = knnForSensor(train, test, w)\n",
    "        print('calculated knn results for ', key)\n",
    "    \n",
    "    votingResults = voteOnResults(knnResults)\n",
    "    print('calculated votingResults')\n",
    "\n",
    "    return classification_report(testStrides[list(testStrides.keys())[0]][:,-1],votingResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def k_means_clust(data, num_clust, num_iter, w=5):\n",
    "    centroids=random.sample(data,num_clust)\n",
    "    counter=0\n",
    "    for n in range(num_iter):\n",
    "        counter+=1\n",
    "        assignments={}\n",
    "        #assign data points to clusters\n",
    "        for ind,i in enumerate(data):\n",
    "            min_dist=float('inf')\n",
    "            closest_clust=None\n",
    "            for c_ind,j in enumerate(centroids):\n",
    "                if LB_Keogh(i,j,5)<min_dist:\n",
    "                    cur_dist=DTWDistance(i,j,w)\n",
    "                    if cur_dist<min_dist:\n",
    "                        min_dist=cur_dist\n",
    "                        closest_clust=c_ind\n",
    "            assignments.setdefault(closest_clust,[])\n",
    "            assignments[closest_clust].append(ind)\n",
    "    \n",
    "        #recalculate centroids of clusters\n",
    "        for key in assignments:\n",
    "            clust_sum=np.zeros(len(data[0]))\n",
    "            for k in self.assignments[key]:\n",
    "                clust_sum=np.add(clust_sum,data[k])\n",
    "            centroids[key]=[m/len(assignments[key]) for m in clust_sum]\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Clustering\n",
    "\n",
    "Expected clustering data input:  \n",
    "arrays of time series for train and test.\n",
    "\n",
    "Since we have multiple time series per stride (multiple sensors), we will put the time series into a dict with sensor keys and one array with all series each. The order is thus important and has to stay consistent to identify complete strides again.\n",
    "A majority vote will be performed on the sensors in the end to cluster the strides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRatio = 0.7 # share of data that should go into training. e.g. 0.7: 70% training, 30% testing\n",
    "\n",
    "def initializeSensorDict(strides):\n",
    "    return {column:[] for column in strides['normal'][0]}\n",
    "\n",
    "def listDictToNumpyArrayDict(dictionary):\n",
    "    for key in dictionary:\n",
    "        dictionary[key] = np.array(dictionary[key])\n",
    "    return dictionary\n",
    "\n",
    "def createSensorNumpyArray(stride, sensor, exerciseNumber):\n",
    "    clusterLabel = float(exerciseNumber + 1)\n",
    "    strideSensorWithLabel = stride[sensor].append(pd.Series([clusterLabel]), ignore_index=True)\n",
    "    return np.array(strideSensorWithLabel)\n",
    "\n",
    "def getTrainAndTestStrides(labelledStrides):\n",
    "    train = initializeSensorDict(labelledStrides)\n",
    "    test = initializeSensorDict(labelledStrides)\n",
    "\n",
    "    for exerciseNumber, exercise in enumerate(labelledStrides):\n",
    "        trainEndIndex = math.floor(len(labelledStrides[exercise]) * trainRatio)\n",
    "        for stride in labelledStrides[exercise][:trainEndIndex]:\n",
    "            for sensor in stride.columns:\n",
    "                train[sensor].append(createSensorNumpyArray(stride, sensor, exerciseNumber))\n",
    "        for stride in labelledStrides[exercise][trainEndIndex:]:\n",
    "            for sensor in stride.columns:\n",
    "                test[sensor].append(createSensorNumpyArray(stride, sensor, exerciseNumber))\n",
    "    \n",
    "    train = listDictToNumpyArrayDict(train)\n",
    "    test = listDictToNumpyArrayDict(test)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelledStrides = {\n",
    "    'normal': [],\n",
    "    'pelvic displacement': [],\n",
    "    'limping': [],\n",
    "    'shuffling': [],\n",
    "    'small steps': [],\n",
    "    'insecure walking': []\n",
    "}\n",
    "\n",
    "dataParentPath = os.path.join('..', 'data', '24-06-19')\n",
    "subjectPaths = [folderTuple[0] for folderTuple in os.walk(dataParentPath)][1:]\n",
    "\n",
    "for subjectPath in subjectPaths:\n",
    "    print(subjectPath)\n",
    "    loadedMeasurement = loadSyncedMeasurements(subjectPath)\n",
    "    if (len(loadedMeasurement) == expectedExerciseCount + 1):\n",
    "        for i, exercise in enumerate(loadedMeasurement[1:]):\n",
    "            strides = splitExerciseIntoStrides(exercise)[1:]\n",
    "            print(list(labelledStrides.keys())[i], len(strides))\n",
    "            labelledStrides[list(labelledStrides.keys())[i]] += normalizeStrides(strides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exerciseCount = 6\n",
    "iterationNumber = 10\n",
    "windowSize = 4\n",
    "\n",
    "trainStrides, testStrides = getTrainAndTestStrides(labelledStrides)\n",
    "clusterResults = {key:None for key in trainStrides}\n",
    "    \n",
    "print(knn(trainStrides, testStrides, windowSize))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
